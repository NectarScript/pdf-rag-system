# ğŸ“„ PDF RAG AI â€” Multi-Document Chat System

A full-stack Retrieval-Augmented Generation (RAG) system that enables users to upload multiple PDFs and interact with them using a Groq-powered Large Language Model (LLM).

This project combines semantic search, metadata filtering, session memory, and streaming responses to deliver a ChatGPT-like experience grounded strictly in document content.

---

## ğŸš€ Overview

This system allows users to:

- Upload multiple PDF documents
- Perform semantic search over document contents
- Chat with documents using contextual retrieval
- Stream AI-generated responses in real time
- Maintain session-based chat history
- Filter responses by specific file name

The architecture cleanly separates retrieval and generation to ensure responses are grounded in indexed document context.

---

## ğŸ—ï¸ Architecture

### ğŸ”¹ Backend
- **FastAPI**
- **Haystack (FAISSDocumentStore)**
- **SentenceTransformers (Embeddings)**
- **Groq LLM API**
- **SQLite metadata storage**
- Streaming response generator

### ğŸ”¹ Frontend
- **Next.js 14 (App Router)**
- **Tailwind CSS**
- **Framer Motion**
- **shadcn/ui**
- Dark/Light mode UI

---

## ğŸ§  How It Works

1. User uploads one or more PDFs.
2. Each PDF is:
   - Parsed
   - Chunked
   - Embedded using SentenceTransformers
   - Stored in FAISS with metadata
3. When a user asks a question:
   - Hybrid retrieval fetches relevant document chunks
   - Optional metadata filtering narrows results by file
   - Context is constructed
   - Prompt is sent to Groq LLM
   - Response is streamed back to frontend
4. Chat history is stored per session.

The model is instructed to answer strictly using provided document context.

---

## âœ¨ Features

- âœ… Multi-PDF indexing
- âœ… Hybrid dense retrieval
- âœ… Metadata-based filtering (`file_name`)
- âœ… Session-based chat memory
- âœ… Streaming LLM responses
- âœ… Persistent FAISS index
- âœ… Strict hallucination control
- âœ… Modern animated UI

---

## ğŸ“‚ Project Structure

pdf-rag/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â”œâ”€â”€ ingestion/
â”‚ â”‚ â”œâ”€â”€ retrieval/
â”‚ â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â””â”€â”€ main.py
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â””â”€â”€ .gitignore
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ components/
â”‚ â”œâ”€â”€ lib/
â”‚ â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md


---

## âš™ï¸ Backend Setup

### 1ï¸âƒ£ Create Virtual Environment

```bash
cd backend
python -m venv venv
venv\Scripts\activate
2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Create Environment File

Create a .env file inside backend/:

GROQ_API_KEY=your_groq_api_key_here


âš ï¸ Do not commit .env to GitHub.

4ï¸âƒ£ Run Backend
uvicorn app.main:app --reload

Backend runs at:

http://127.0.0.1:8000


Swagger documentation:

http://127.0.0.1:8000/docs

ğŸ’» Frontend Setup
cd frontend
npm install
npm run dev

Frontend runs at:

http://localhost:3000
